# MentalBench-100k & MentalAlign-70k: Reliable Evaluation of LLMs in Mental Health Dialogue  

This repository contains the datasets, code, and evaluation pipeline for the paper **"When Can We Trust LLMs in Mental Health? Large-Scale Benchmarks for Reliable Dialogue Evaluation and Alignment"**.  

<img width="2870" height="1588" alt="framework" src="https://github.com/user-attachments/assets/50fb664d-beb4-4c07-8d4d-d392e15c2a36" />


## Overview  

We introduce two complementary benchmarks and an evaluation framework for systematically evaluating LLMs in mental health support:  

- **MentalBench-100k**: A benchmark of 10,000 context–response dialogues and 100,000 additional replies generated by nine diverse LLMs. We generated responses using diverse LLMs to enable a critical evaluation, given the increasing exploration of their use in real-world scenario therapeutic settings.
- **MentalAlign-70k**: 70,000 ratings across 7 attributes from both human clinical experts and LLM judges, enabling the first large-scale human–AI comparison of evaluation reliability.
- **Affective–Cognitive Agreement Framework**: A dual reliability framework with a three-pillar (consistency, agreement, bias), and a reliability classification scheme. This framework investigates when reliability can be trusted versus when human oversight is mandatory, providing an evidence-based reliability guidance for mental health AI systems.  

Together, these resources establish a dual-benchmark ecosystem for studying **response generation** and **evaluation alignment** in mental health contexts.  

---

## Key Contributions  

1. **MentalBench-100k Dataset**: A large benchmark of single-session therapeutic dialogues with 9 LLM-generated responses per conversation with 100,000 contexts and responses.  
2. **MentalAlign-70k Dataset**: Dual-axis evaluation (Cognitive Support Score, Affective Resonance Score) with 70,000 ratings from experts and 4 LLM judges.  
3. **Affective–Cognitive Alignment Framework**: Reliability-oriented methodology using Intraclass Correlation Coefficients (ICC), bootstrap confidence intervals, and bias analysis to quantify agreement magnitude and precision.  
4. **Reliability Guidance**: First evidence-based recommendations for when LLM-as-a-judge evaluation can be trusted and when human oversight is essential.  

---

## Repository Structure

```
├── MentalBench-100k/              # Dataset files
│   ├── MentalBench-100k.zip       # Main dataset with conversations and responses
├── MentalAlign-70k/              # Human and LLMs as a judge evaluation results
│   ├──LLMs_as_a_judge/     # LLMs as a judge Evaluation Results
│      ├── claude-3-7-sonnet_llm_judge.csv   # Claude as LLM as a judge
│      ├── gemini-2.5-flash_llm_judge.csv    # Gemini as LLM as a judge
│      ├── gpt-4o-llm_judge.csv      # GPT 4o as LLM as a judge
│      └── o4-mini-llm_judge.csv     # o4-mini as LLM as a judge
│   ├── Human_Judge /         # Human Evaluation Results
│       ├── Human_judge.xlsx  # Humans as a judge
├── code/                          # Implementation code
│   ├── generation/                # LLM response generation scripts
│   ├── evaluation/                # Evaluation framework implementation
│   └── analysis/                  # Analysis and visualization scripts for the ICC Framework
├── results/                       # Evaluation results
│   ├── Human Response Evaluation/        #  Human expert evaluation results
│   ├── ICC Reliability Analysis/         # ICC Analysis from the paper
│   └── Systematic Bias Decomposition/    # Bias Results
├── docs/                          # Documentation
│   ├── evaluation_guidelines.md   # Evaluation criteria and guidelines
│   └── dataset_description.md    # Dataset description and statistics
├── 
└── README.md                      # This file
```



## Dataset  

### MentalBench-100k  

- **Conversations**: 10,000  
- **Responses per conversation**: 1 human + 9 LLMs  
- **Total responses**: 100,000  
- **Conditions covered**: 23 clinically relevant categories (anxiety, depression, relationships, grief, etc.)  
- **Average context length**: 72.6 words  
- **Average response length**: 87.0 words  

**LLMs used**: GPT-4o, GPT-4o-Mini, Claude-3.5-Haiku, Gemini-2.0-Flash, LLaMA-3.1-8B-Instruct, Qwen2.5-7B, Qwen-3-4B, DeepSeek-LLaMA-8B, DeepSeek-Qwen-7B.  

---

### MentalAlign-70k  

- **Ratings**: 70,000 across 1,000 conversations × 10 responses × 7 attributes  
- **Judges**: 3 human experts + 4 LLMs (Claude-3.7-Sonnet, GPT-4o, GPT-4o-Mini, Gemini-2.5-Flash)  
- **Attributes**:  
  - **Cognitive Support Score (CSS)**: Guidance, Informativeness, Relevance, Safety  
  - **Affective Resonance Score (ARS)**: Empathy, Helpfulness, Understanding  

---

## Evaluation Framework  

- **ICC Analysis**: Agreement and consistency between human and LLM judges.  
- **Bootstrap Confidence Intervals**: Quantify precision of reliability estimates.  
- **Bias Detection**: Attribute- and model-specific inflation analysis.  
- **Reliability Categories**:  
  - Good Reliability (GR)  
  - Moderate Validation Needed (MV)  
  - Limited Reliability (LR)  

This framework reveals where automated evaluation is reliable (e.g., Guidance, Informativeness) and where human oversight is mandatory (e.g., Empathy, Safety, Relevance).  

---

## Results (Highlights)  

- **High-capacity models** (GPT-4o, Gemini-2.0-Flash) consistently outperform smaller open-source systems.  
- **Empathy & Helpfulness** show deceptively high scores but wide uncertainty, requiring caution.  
- **Safety & Relevance** exhibits systematically poor reliability across judges.  
- **LLM Judges** systematically inflate ratings (+0.4–0.8 on affective attributes).  

---


## Usage

### Setup

```bash
# Clone the repository
git clone https://github.com/your-username/MentalBench-Align.git
cd mentalbench-10

# Install dependencies
pip install -r requirements.txt
```

### Running Evaluations

```python
# Generate LLM responses
python Codes/generation/generate_responses.py

# Run LLM-as-a-judge evaluation
python Codes/evaluation/llm_judge_evaluation.py

# Calculate ICC analysis factors
python Codes/analysis/icc_corrected_with_bootstrap_ci.py
```

##  Citation
```bibtex

  title={When Can We Trust LLMs in Mental Health? Large-Scale Benchmarks for Reliable LLM Evaluation},
  author={Badawi, Abeer and Rahimi, Elahe and Laskar, Md Tahmid Rahman and others},
  year={2025},
  journal={arXiv preprint arXiv:XXXX.XXXXX}
}

## Ethical Considerations

This work involves sensitive mental health data and AI-generated responses. Key considerations:

- All datasets were publicly available and anonymized
- Models are not intended to replace human therapists
- Potential for demographic and cultural biases
- Emotional burden on human annotators acknowledged
- Focus on responsible AI deployment in mental health

