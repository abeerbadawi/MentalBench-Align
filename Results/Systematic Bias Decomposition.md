### ğŸ“ RQ3: Systematic Bias Decomposition â€” Where Do LLM Judges Drift from Human Standards?

While RQ2 focused on *reliability* (i.e., consistency and precision), RQ3 investigates **systematic bias** â€” persistent deviations between LLM ratings and human expert scores.

We distinguish between:
| Error Type | What It Means | Can It Be Fixed? |
|-----------|--------------|------------------|
| âœ… **Systematic Bias** | Consistent over/under-scoring patterns | Potentially correctable via calibration |
| âŒ **Random Error** | Unstable, unpredictable variation | Not easily fixable â€” indicates fundamental unreliability |

---

### âœ… How We Analyzed Bias

For each attribute, we computed:
| Metric | Description |
|--------|------------|
| **Human Mean** | Average human rating (baseline) |
| **LLM Mean** | Average rating by the judge |
| **Bias** | LLM âˆ’ Human (positive = inflation) |
| **MSE** | Error magnitude vs human truth |

ğŸ“Œ We examined 4 judges Ã— 7 attributes = 28 judgeâ€“attribute bias cases.

---

### ğŸ¯ Key Findings

| Dimension | Trend | Interpretation |
|-----------|--------|----------------|
| ğŸ§  **Cognitive attributes** (Guidance, Informativeness) | Small/moderate bias (+0.25 to +0.46) | âœ… Suitable for **calibration-based correction** |
| ğŸ’› **Affective attributes** (Empathy, Helpfulness) | Inflated bias (+0.40 to +0.80) | âš ï¸ Risk of exaggerated empathy â†’ â€œfalse warmthâ€ |
| âš ï¸ **Safety & Relevance** | Modest bias (+0.18 to +0.39) but unreliable | âŒ Bias correction not enough due to poor reliability (from RQ2) |

---

### ğŸ“Š Bias Table (Human vs LLM Means, Bias & MSE)

| **Attribute** | **Claude (Human / LLM / Bias / MSE)** | **GPT-4o** | **Gemini** | **o4-mini** |
|--------------|----------------------------------------|-----------|-----------|-----------|
| Guidance | 3.742 â†’ 3.990 â†’ +0.248 â†’ 0.923 | 3.656 â†’ 4.427 â†’ +0.771 â†’ 1.513 | 3.667 â†’ 4.154 â†’ +0.486 â†’ 1.368 | 3.680 â†’ 4.120 â†’ +0.440 â†’ 1.114 |
| Informativeness | 4.032 â†’ 3.931 â†’ âˆ’0.101 â†’ 0.829 | 3.951 â†’ 4.412 â†’ +0.461 â†’ 0.958 | 3.956 â†’ 4.071 â†’ +0.115 â†’ 1.032 | 3.963 â†’ 3.819 â†’ âˆ’0.144 â†’ 0.846 |
| Relevance | 4.520 â†’ 4.574 â†’ +0.054 â†’ 0.999 | 4.478 â†’ 4.867 â†’ +0.389 â†’ 0.780 | 4.484 â†’ 4.886 â†’ +0.401 â†’ 0.880 | 4.487 â†’ 4.917 â†’ +0.431 â†’ 0.804 |
| Safety | 4.734 â†’ 4.852 â†’ +0.118 â†’ 0.521 | 4.714 â†’ 4.932 â†’ +0.218 â†’ 0.451 | 4.716 â†’ 4.924 â†’ +0.208 â†’ 0.550 | 4.716 â†’ 4.967 â†’ +0.251 â†’ 0.534 |
| Empathy | 4.046 â†’ 4.687 â†’ +0.641 â†’ 1.181 | 3.958 â†’ 4.775 â†’ +0.817 â†’ 1.391 | 3.992 â†’ 4.695 â†’ +0.703 â†’ 1.310 | 3.991 â†’ 4.572 â†’ +0.581 â†’ 1.117 |
| Helpfulness | 3.972 â†’ 4.399 â†’ +0.427 â†’ 0.946 | 3.869 â†’ 4.538 â†’ +0.669 â†’ 1.130 | 3.896 â†’ 4.643 â†’ +0.747 â†’ 1.354 | 3.888 â†’ 4.362 â†’ +0.474 â†’ 0.912 |
| Understanding | 4.511 â†’ 4.543 â†’ +0.031 â†’ 1.084 | 4.472 â†’ 4.821 â†’ +0.349 â†’ 0.769 | 4.477 â†’ 4.875 â†’ +0.397 â†’ 0.934 | 4.478 â†’ 4.780 â†’ +0.303 â†’ 0.758 |

---

### ğŸ“Œ Interpretation Summary

| Attribute Type | Typical Bias | Fixable? | Implication |
|----------------|-------------|---------|-------------|
| ğŸ§  Cognitive | Small (+0.25â€“0.46) | âœ… Yes | Calibration possible |
| ğŸ’› Affective | High (+0.58â€“0.82) | âš ï¸ Risky | Could cause false trust |
| ğŸš¨ Safety/Relevance | Moderate (+0.18â€“0.39) but low reliability | âŒ No | Requires human oversight |

---

### âœ… Takeaway

> *LLM judges donâ€™t just â€œdrift randomlyâ€â€”they show predictable over-optimism, especially in emotionally sensitive attributes like empathy and helpfulness. While cognitive scoring can be calibrated, affective and safety-critical assessments require stricter human supervision.*

